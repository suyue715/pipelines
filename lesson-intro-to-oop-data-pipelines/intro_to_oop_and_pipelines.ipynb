{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Binarizer, PolynomialFeatures, \\\n",
    "StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, \\\n",
    "FeatureUnion, make_union\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Oriented Programming\n",
    "\n",
    "In Python, everything is an \"object\" of some type. This is the basis of what is known as Object Oriented Programming (OOP). Broadly, the focus within an OOP language is on objects -- what they are and what we can do with them. Some examples include:\n",
    "\n",
    "- A `list` -- we can add, sum, or remove elements in a list, or do something to each element there\n",
    "- A Panda's `DataFrame` -- we can slice and manipulate each column, move them to new DataFrames, or reduce them to single numbers\n",
    "- A sklearn `DecisionTreeClassifier` -- we can create splits for each of the nodes, access and view them, refit it to new data, or use its internal values to predict or score new data\n",
    "\n",
    "Some things that are common across all of these objects (and broadly, across this programming paradigm):\n",
    "\n",
    "1. Everything we've defined in our language has certain things they can and can't do -- in other words, there's inherent behavior to one type of object versus another.\n",
    "2. Almost every object is mutable in some fashion -- we can add or modify values in all three of the examples above. \n",
    "3. Our focus as developers in an OOP language like Python is what we want to do to each of those objects and how to connect or modify them.\n",
    "\n",
    "Contrast this to the other broad paradigm, Functional Programming. In Functional Programming, our data is (typically) immutable -- we are not changing the underlying values or doing anything to them. Instead, through the use of functions and control flow, we represent that data differently. \n",
    "\n",
    "In other words (very broadly), OOP programs are interested in changing an object where as FP are interested in the evaluation of functions. Most languages are not purely one or the other -- in fact, many offer elements of both paradigms. \n",
    "\n",
    "> Popular Data Science languages `R` and `Julia` typically have stronger functional programming elements than Python!\n",
    "\n",
    "An example of Pythonic OOP programming to add 2 to every number in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5]\n",
    "my_list = [x + 2 for x in my_list]\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to that would be a functional programming approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7] [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def add2(value):\n",
    "    return value + 2\n",
    "\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "print(list(map(add2, my_list)), my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the OOP approach to the problem, the result is a **new** or **modified** object. In the FP approach to the problem, the result is the output of `my_list` (which is not modified) through `map` applying `add2` to each value, then `list` to convert that mapping into a list representation.\n",
    "\n",
    "While there are many other differences between functional and object oriented programming paradigms, here's what to take away from this:\n",
    "\n",
    "1. Everything in Python is an object.\n",
    "2. Much of our work as Python developers will involve creating, using, or modifying the contents of objects.\n",
    "3. Functional programming involves passing (immutable) data through multiple functions to represent it in our desired way. While this is achievable in Python (and most programming languages), we are _typically_ solving programs using an object-oriented paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes in Python \n",
    "\n",
    "A class is a type of object. You can think of a class definition as a sort of \"blueprint\" that specifies the construction of a new object when instantiated.\n",
    "\n",
    "One quick example is to imagine that I have two sets of data and I fit a `DecisionTreeClassifier` to each of them:\n",
    "\n",
    "```python\n",
    "df1 = pd.DataFrame(...)\n",
    "df2 = pd.DataFrame(...)\n",
    "\n",
    "dt1 = DecisionTreeClassifier()\n",
    "dt1.fit(df1, target)\n",
    "\n",
    "dt2 = DecisionTreeClassifier()\n",
    "dt2.fit(df2, target)\n",
    "```\n",
    "\n",
    "Both `dt1` and `dt2` have the same underlying blueprint. Calling `.fit()` on both will start the process for fitting a decision tree. Calling `.score()` will call the same scoring method for both, etc. They are both the _same_ kind of object and can _do_ the same kind of things.\n",
    "\n",
    "**However**, both have different _attributes_. If we called `.tree_` on both, we would see a different underlying decision tree, with different splits, different nodes, and different data for each. In other words, while they both can _do_ the same things, they have different internal values. \n",
    "\n",
    "Instantiating a class lets us create a unique copy of that class -- it can do all the same things that every other member of its class can do (such as `.fit()`, `.score()`, etc.) but lets us keep unique internal values (or attributes) to that specific instance of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check For Understanding (10-15 Minutes)\n",
    "\n",
    "In a small group (2-4 people), try to work through the following example:\n",
    "1. Pick a pet (like a cat, or a dog, or a lizard)\n",
    "2. What are things that that every member of that pet can do (such as eat)? \n",
    "3. What are attributes of a specific pet that might differ across each of the class (such as their name)?\n",
    "4. Are there things that those pets can do that _other_ pets can also do? _Other_ animals? \n",
    "5. Write down a set of the following:\n",
    "    - Things that all animals can do\n",
    "    - Things that members of your pet class can do\n",
    "    - Things that are unique to a specific pet \n",
    "\n",
    "This thought experiment might seem silly, but breaking items up into these questions of shared behavior and distinct attributes underlie how classes work in object oriented programs overall. \n",
    "\n",
    "If your group finishes early, replace _pet_ with the Python `list()` object. What are things that every list can do? If we create a specific list (`[1, 2, 3, 4, 5]`), what attributes of that list are unique to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example: Creating Classes and Inheritance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend that we were creating vehicles. We might begin by creating a generic Vehicle class.\n",
    "\n",
    "> Note: we typically use proper case in Python to denote classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle(object):\n",
    "    pass\n",
    "\n",
    "vehicle = Vehicle()\n",
    "vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with `class` to denote that what follows is a class, followed by the name of the class. `(object)` denotes that we are inheriting things from the `object` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `object`? `object` is basically the grandfather object of all other objects in Python. Here we're just letting Python know that `Vehicle` should be able to do everything that `object` can (which is not much!) We'll dive a little more into what that means in a second.\n",
    "\n",
    "Next, let's give `Vehicle` some attributes: a `number_of_wheels`, a `name`, a `current_speed`, and a `max_speed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.number_of_wheels = 4\n",
    "        self.name = 'KITT'\n",
    "        self.current_speed = 0\n",
    "        self.max_speed = 60\n",
    "        \n",
    "vehicle = Vehicle()\n",
    "print(vehicle,\n",
    "     vehicle.number_of_wheels,\n",
    "     vehicle.name,\n",
    "     vehicle.current_speed,\n",
    "     vehicle.max_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we did here:\n",
    "\n",
    "1. Every class can have internal functions (which we call **methods** that do things in that class. This is how we define what an object can do!)\n",
    "2. We define a special function known as `__init__(self)` (the `__` is pronounced \"dunder\" for double underscore). This method runs as soon as a specific instance of the object is created (we call that **instantiating**). \n",
    "> For all of the methods we assign to an object, their first argument will be `self` to tell the class to look internally for attributes or variables.\n",
    "3. Inside that function, we detail things that we might want to have populated as soon as the object exists, like its name and max speed. \n",
    "4. We denote the **attributes** or things specific to **one instance** of the object with `self.` -- this points back to whatever current instance of the object exists.\n",
    "5. Once we instantiate the object (`vehicle = Vehicle()`), Python calls the internal `__init__(self)` and sets up anything that is inside of there.\n",
    "\n",
    "We can also have certain attributes be defined by the user when they instantiate the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle(object):\n",
    "    \n",
    "    def __init__(self, current_speed, name='KITT'):\n",
    "        self.number_of_wheels = 4\n",
    "        self.name = name\n",
    "        self.current_speed = current_speed\n",
    "        self.max_speed = 60\n",
    "        \n",
    "vehicle = Vehicle(25, name='Big Blue Van')\n",
    "\n",
    "print(vehicle,\n",
    "     vehicle.number_of_wheels,\n",
    "     vehicle.name,\n",
    "     vehicle.current_speed,\n",
    "     vehicle.max_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have changed the arguments that go into `__init__()` to include an argument for `current_speed` and a keyword argument for `name`. We then assign whatever values are passed into `Vehicle` _when it is instantiated_ to those attributes:\n",
    "\n",
    "```python\n",
    "def __init__(self, name, current_speed)\n",
    "    self.name = name # name comes through the class instantiation\n",
    "    self.current_speed = current_speed # same here\n",
    "```\n",
    "\n",
    "Using keyword arguments means that the user doesn't have to define them at the start: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_car = Vehicle(10)\n",
    "\n",
    "print(vehicle.name, my_car.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, using regular arguments will throw up an error if they are not supplied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_car = Vehicle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in a couple of extra methods to do things with any vehicle we want:\n",
    "\n",
    "1. A method to change the current speed of the vehicle\n",
    "2. A method to check if our speed is too fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vehicle(object):\n",
    "    \n",
    "    def __init__(self, current_speed, name='KITT'):\n",
    "        self.number_of_wheels = 4\n",
    "        self.name = name\n",
    "        self.current_speed = current_speed\n",
    "        self.max_speed = 60\n",
    "        \n",
    "    # New methods here \n",
    "    \n",
    "    def set_current_speed(self, speed):\n",
    "        self.current_speed = speed\n",
    "        \n",
    "    def check_speed(self):\n",
    "        if self.current_speed >= self.max_speed:\n",
    "            print('Woah! You are driving at {}'.format(self.current_speed))\n",
    "            print('Warning! Too fast! Slowing you down!')\n",
    "            self.current_speed = self.max_speed\n",
    "        else:\n",
    "            print('You are driving at {}!'.format(self.current_speed))\n",
    "            print('Thank you for driving safely!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `__init__()`, we add in two more functions, using `def` to define them and passing in `self` as the first argument. If we want extra information from the end user for that call, we can also set that up as additional arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cab = Vehicle(95, name='Death Cab')\n",
    "\n",
    "death_cab.check_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.set_current_speed()` to go slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "death_cab.set_current_speed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then try `.check_speed()` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cab.check_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if `death_cab` only had 3 wheels? We could set this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cab.number_of_wheels = 3\n",
    "print(death_cab.number_of_wheels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But different vehicles inherently have different numbers of wheels. While we could create a new class for, say, `Bicycles` and another for `18WheelerTrucks`, that seems like a lot of extra coding. \n",
    "\n",
    "Thankfully, classes allow for inheritance. This means that we can use an existing class as the start of a blueprint and modify as we need to. Let's make a `Bicycle` class that inherits from `Vehicle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bicycle(Vehicle):\n",
    "    \n",
    "    def __init__(self, current_speed, name='Bikey'):\n",
    "        super().__init__(current_speed, name)\n",
    "        self.number_of_wheels = 2\n",
    "        self.max_speed = 15\n",
    "        \n",
    "    def ring_bell(self):\n",
    "        print('BRRRRRING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we inherit from a class, we can overwrite a method that it already has (like we do with `__init__()` above) or we can add a new method (like `ring_bell()`).\n",
    "\n",
    "`super()` is a special function within a class. It lets us go to the class that we are inheriting from and call a method that we may have overwritten. \n",
    "\n",
    "In this example, we initalize all four attributes in `Vehicle` by using `super()` and passing in `current_speed` and `name`, **THEN** we reset `number_of_wheels` and `max_speed` as is appropriate for Bicycle.\n",
    "\n",
    "> Note, in Python 3, `super()` already implicitly passes in `self`, so we don't need to worry about adding it.\n",
    "\n",
    "`Bicycle` has access to anything defined in `Vehicle`, but has a specific default for `max_speed` and `wheels`, plus a method `ring_bell()` that `Vehicle` does not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car = Vehicle(0)\n",
    "bicycle = Bicycle(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car.check_speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.check_speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.number_of_wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.ring_bell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car.ring_bell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding (20 Minutes)\n",
    "\n",
    "Individually, please attempt the following:\n",
    "\n",
    "1. Modify the `Vehicle` class to have:\n",
    "    1. A `seats` attribute that lists the number of seats. This value should be set to 4.\n",
    "    2. A `currently_riding` attribute that will hold the names of whoever is currently in the vehicle. This value should be set to an empty list (`[]`).\n",
    "    3. Three methods: \n",
    "        - `add_passenger()`: this should take the name of the passenger in as an argument. Before it adds the passenger to the `currently_riding`, it should check how many people are currently riding and, if it is the same value as the number of seats that the vehicle can support, it should tell you that the vehicle is full.\n",
    "        - `let_passenger_off()`: this should take the name of a passenger in as an argument. It should check to see if that name exists in the list of `currently_riding`. If the name exists there, it should remove it from that list. If the name does not exist there, it should warn the user.\n",
    "        - `current_passengers()`: this should print out each element in the `currently_riding` attribute.\n",
    "2. Write some code that tests whether the changes you have made to `Vehicle` are working successfully.\n",
    "    > This can be very informal, but the key ideas are to know what it should look like if each of your additions worked successfully, then try and give it input to make sure that it works correctly. \n",
    "    > For example, if I wanted to test that `number_of_wheels` was added successfully I might write something like:\n",
    "```python\n",
    "# I think if I call number_of_wheels it should return back the number of wheels, which in this case is 4\n",
    "vehicle.number_of_wheels\n",
    ">>> 4\n",
    "# Success!\n",
    "```\n",
    "3. Modify the `Bicycle` class to:\n",
    "    1. Have a default `seats` attribute of 1.\n",
    "4. Create a new `Train` class that:\n",
    "    1. Inherits from the `Vehicle` class.\n",
    "    2. Sets the `number_of_wheels` attribute to 12.\n",
    "    3. Sets the `number_of_seats` attribute to 24. \n",
    "    \n",
    "This is a pretty intimidating check for understanding, so feel free to reach out to your classmates, the Slack, and your local instructors as well, if you feel lost. There is solution code in this repository as well, but I highly encourage you to spend 20 minutes pushing yourself to try and develop these skills.\n",
    "\n",
    "Question 2 can be coded up as informally or formally as you wish, but learning how to test your code to ensure that its behavior is as you expect can be a really key skill (and is one I do frequently when I code) -- just because it looks like it works doesn't mean that it **actually** works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As developers, we will typically build out classes to handle the organization and manipulation of data -- such as parameters, distances, etc. We've already seen cases like this with both `DataFrame` and modeling libraries like `DecisionTreeClassifier`.\n",
    "\n",
    "What we will usually do with classes is to help extend or build out modules that already exist. Our next step will be to build out a base library within the `preprocessing` module in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP / Classes Applied: Creating a `FeatureExtractor` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this applied exercise, we're going to create a `FeatureExtractor` class using sklearn objects. Our goal is to create a sklearn object with a `.fit()` and `.transform()` method that, when given a Pandas DataFrame, will extract a specific column by column name. \n",
    "\n",
    "Why would we want this class to exist? We will use it in our next section on data pipelines and the sklearn `Pipeline` object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn `mixin` object\n",
    "\n",
    "**Note**: This portion is taking advantage of some years of experience working with scikit-learn and is intended to illustrate how a developer might use their insight into classes to help extend a library. We are _not_ expecting you, at this point, to have the knowledge of sklearn or the experience with Python to be able to apply this to a different case down the line. Use this as an example of workflow, not an expectation for skills development.\n",
    "\n",
    "We're going to make use of a [`mixin`](https://en.wikipedia.org/wiki/Mixin) class. These classes usually exist in larger libraries and provide a framework for a set of expected behaviors. \n",
    "\n",
    "Within sklearn, there are a set of mixin classes kept in the [`base`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.base) library. These are the backbones of all of the modules we've used to date. For example:\n",
    "\n",
    "- [`LogisticRegression`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py#L962) inherits three mixin classes (`BaseEstimator`, `LinearClassifierMixin`, and `SparseCoefMixin`) -- each of these classes provide standardized methods and structure for the `LogisticRegression` class to work.\n",
    "- [`StandardScaler`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/preprocessing/data.py#L461) inherits from `BaseEstimator` and `TransformerMixin`, each of which give it standardized methods as well.\n",
    "\n",
    "#### What this means\n",
    "\n",
    "Sklearn's preprocessing libraries all inherit from the `BaseEstimator` and `TransformerMixin` libraries. This will give us access to the following methods:\n",
    "- `get_params()`\n",
    "- `set_params()`\n",
    "- `fit_transform()`\n",
    "\n",
    "in exactly the way that sklearn expects to see. It will be up to us to  create the object, and then define our own `.fit()` and `.transform()` methods. \n",
    "\n",
    "#### Why both `.fit()` and `.transform()`?\n",
    "\n",
    "Our class to extract a specific column only needs to find a column by name and extract it -- it never needs to \"remember\" anything. Our `.fit()` method won't do very much at all. However, other things in sklearn are going to expect to see a `.fit()` method, so we will include one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create some fake data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_dict({'a': [1, 2, 3, 4, 5], \n",
    "                               'b': [6, 7, 8, 9, 10]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's set up the basic framework for our class -- we'll create a `.fit()` and a `.transform()` method that do nothing, then we will fill them in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self):\n",
    "        print('Called the fit method')\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        print('Called the transform method')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtractor()\n",
    "fe.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the inheritence, we also have access to `fit_transform()` which will do a `fit()` then immediately `transform()` the same data, all in one step. However..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to have a `X` variable passed in when we call `.fit_transform()` -- this is standard templating across sklearn. The classes are looking for the features which, internally, they will call `X` (there are also a smaller number of classes where they are looking for the predictors, which they'll internally call `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, because `.fit_transform()` is calling for an `X`, once we gave it one, it tries to immediately pass it to *our* `.fit()`. *Our* `.fit()` method right now does not take an `X` and so throws an exception. We'll fix that shortly.\n",
    "\n",
    "Do the other methods inherited from `BaseEstimator` work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they do!\n",
    "\n",
    "Let's refactor our `FeatureExtractor` class to take in an `X` in our `.fit()` and `.transform()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X): # new line\n",
    "        print('Called the fit method')\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X): # new line\n",
    "        print('Called the transform method')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtractor()\n",
    "fe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be happening here? If we look at the `TransformerMixin` code for what happens when we call `.fit_transform()`, we see the following lines for `.fit_transform()`:\n",
    "\n",
    "```python\n",
    "def fit_transform(self, X, y=None, **fit_params):\n",
    "        ...\n",
    "        if y is None:\n",
    "            # fit method of arity 1 (unsupervised transformation)\n",
    "            return self.fit(X, **fit_params).transform(X)\n",
    "        else:\n",
    "            # fit method of arity 2 (supervised transformation)\n",
    "            return self.fit(X, y, **fit_params).transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's calling `.transform(X)` on whatever comes out of `self.fit(X)` -- we're not passing anything out of `.fit()` and so `.transform()` fails. How do we fix this?\n",
    "\n",
    "What we can do is instead of `pass`, we can use a construction called `return self` -- this will mean that if we call `.fit()`, we'll just return our own class back. This will let us **chain commands** together in the way that sklearn expects.\n",
    "\n",
    "**Note**: This is behavior that does not exist in every library (because each library is different, coded by different developers, etc.) -- it's good to know but will not 100% be a pattern in every library you see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X): \n",
    "        print('Called the fit method')\n",
    "        return self # new line\n",
    "    \n",
    "    def transform(self, X): \n",
    "        print('Called the transform method')\n",
    "        pass\n",
    "    \n",
    "fe = FeatureExtractor() # need to reinstantiate with new code\n",
    "fe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now all we need to do is modify fit and transform to do what we want. \n",
    "\n",
    "So, let's take a step back and think about what we want to do:\n",
    "\n",
    "1. when we instantiate `FeatureExtractor`, we want to be able to give it a column name to look for. \n",
    "2. when we call `.fit()`, we don't need to do anything.\n",
    "3. when we call `.transform()`, we want `FeatureExtractor` to look at what it's been given and return back a numpy array with just those values\n",
    "    > Why a numpy array and not a `DataFrame`? Numpy arrays tend to work more easily with other sklearn components. \n",
    "    \n",
    "Since we don't need to do anything for step 2, let's get step 1 targeted first. We know that when we instantiate a class, it will run whatever is in the `__init__()` method, and this is where we can establish start-of-life attributes. Let's add in a `__init__()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        print('Initialized Class') # new line\n",
    "        self.column = column # new line\n",
    "        \n",
    "    def fit(self, X): \n",
    "        print('Called the fit method')\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X): \n",
    "        print('Called the transform method')\n",
    "        pass\n",
    "    \n",
    "fe = FeatureExtractor('b') \n",
    "fe.fit_transform(df)\n",
    "print(fe.column) # new line\n",
    "print(fe.get_params()) # new line\n",
    "print(fe.set_params()) # new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a couple of new things have happened -- we can now call the `.column` attribute of the class itself, and the `.get_params()` and `.set_params()` methods (which know what to look for) are returning information about our new attribute as well. \n",
    "\n",
    "Let's change the `.transform()` method -- we'll also add in a little boilerplate code so that if we do pass in an `X` and a `y`, it can accept the `y` and then safely ignore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        print('Initialized Class') \n",
    "        self.column = column \n",
    "        \n",
    "    def fit(self, X, y=None): # new line\n",
    "        print('Called the fit method')\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None): # new line\n",
    "        print('Called the transform method')\n",
    "        return X[[self.column]].values # new line\n",
    "    \n",
    "fe = FeatureExtractor('b') \n",
    "fe.fit_transform(df)\n",
    "print(fe.column) \n",
    "print(fe.get_params())\n",
    "print(fe.set_params()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything ran successfully, but that _should_ have transformed our data. Let's make one tweak (because we are now returning something in `.transform()`, we want to print out what comes out of the method as opposed to sending it off into the ether)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        print('Initialized Class') \n",
    "        self.column = column \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print('Called the fit method')\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Called the transform method')\n",
    "        return X[[self.column]].values \n",
    "    \n",
    "fe = FeatureExtractor('b') \n",
    "print(fe.fit_transform(df)) # new line\n",
    "print(fe.column) \n",
    "print(fe.get_params())\n",
    "print(fe.set_params()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's our `b` column from our fake `DataFrame`. We've got this class successfully made!\n",
    "\n",
    "Our final edit will be to remove the print statements inside of the class, now that we know that it is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[[self.column]].values \n",
    "    \n",
    "fe = FeatureExtractor('b') \n",
    "print(fe.fit_transform(df))\n",
    "print(fe.column) \n",
    "print(fe.get_params())\n",
    "print(fe.set_params()) \n",
    "\n",
    "# we can now even do this\n",
    "fe.fit(df)\n",
    "column = fe.transform(df)\n",
    "print(df, '\\n', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this custom class moving forward in our `Pipeline` discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines and sklearn `Pipeline` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember from our discussion a couple of weeks ago, we talked about a typical Data Science workflow:\n",
    "\n",
    "![](./images/model_development.png)\n",
    "\n",
    "In this sort of scheme, we can imagine a couple of steps happening across the model fitting and model deployment processes:\n",
    "\n",
    "#### Model Generation\n",
    "\n",
    "1. Fit transformations to training data\n",
    "2. Transform training data\n",
    "3. Fit model to transformed training data\n",
    "4. Predict / Predict Probabilities / Score Model \n",
    "\n",
    "#### Model Deployment\n",
    "\n",
    "1. Transform incoming data based on previous fit\n",
    "2. Predict / Predict Probabilities / Score Previously fit Model\n",
    "\n",
    "In other words, **when fitting**, we're going to have a set of steps to do to fit data transformations. Then, we'd like to transform our data, doing all of those in the correct order. Then we'd like to fit our model on that transformed data. Then we'd like to do stuff with that model. **When deployed**, we want to reproduce just the transformation and predictions steps.\n",
    "\n",
    "So, really, we could break this down into something simpler:\n",
    "\n",
    "#### Model Generation\n",
    "1. Data_Transform -- all **fit** steps\n",
    "2. Data_Transform -- all **transform** steps\n",
    "3. Model -- all **fit** steps\n",
    "4. Model -- all **predict** steps\n",
    "\n",
    "#### Model Deployment\n",
    "1. Data_Transform -- all **transform** steps\n",
    "2. Model -- all **predict** steps\n",
    "\n",
    "Sklearn has a library (`Pipeline`) that, once you know how it works, is much easier than trying to handle all of these steps by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Steps -- the `Pipeline` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` allows you to set up a list of steps. Once set up, the `Pipeline` will take a set of data and sequentially feed it through the steps, calling fit and transform at each step and passing that result into the next step. At the last step, the `Pipeline` object will do whatever your last command for it is (i.e., if you call .predict() it will return the predictions of whatever you wanted).\n",
    "\n",
    "A `Pipeline` requires a list of steps. These steps are kept in a tuple with a name for the step followed by the step to take. These steps can either be instantiated into their own object or passed in directly to the list.\n",
    "\n",
    "Let's use the Iris data and the following libraries:\n",
    "\n",
    "- `FeatureExtractor` -- the custom sklearn object we created above\n",
    "- `Binarizer` -- to create a cutoff\n",
    "- `KNeighborsClassifier` -- to classify the Iris flowers.\n",
    "\n",
    "We'll begin by using `train_test_split()` to split the data into a training set and a test set. This will be to show off that the `Pipeline` object can be reproducibly used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('datasets/iris.csv')\n",
    "\n",
    "def split_species(val):\n",
    "    if val == 'setosa':\n",
    "        return 0\n",
    "    elif val == 'versicolor':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "iris['species'] = iris['species'].apply(split_species)\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split `species` into its own `y` variable and assign the remaining features to `X`, then train-test split (using 2017 as the `random_state`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = iris['species'].copy()\n",
    "X = iris[[col for col in iris.columns if col !='species']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up the steps. The model we want to make is one where we take the `petal_length` feature, bin it into a dummy variable where the cutoff is the median petal length, and then predict the flower species using that one feature. \n",
    "\n",
    "First, we will list out the steps in sequential order. Each step will in the list will look like this:\n",
    "\n",
    "```python\n",
    "('NAMEOFSTEP', callable())\n",
    "```\n",
    "\n",
    "where the `'NAMEOFSTEP'` is the name of that step (as a string), and the `callable()` is any sklearn object that has a `.fit()` and a `.transform()` method. The **last** item in the list of steps can be a model object with `.score()`, `.predict()`, and `.predict_proba()` as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_steps = [\n",
    "    ('extract_petal_length', FeatureExtractor('petal_length')),\n",
    "    ('cut_off_at_median', Binarizer(X_train['petal_length'].median())),\n",
    "    ('predict_using_knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "print(modeling_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll instantiate a Pipeline object, passing in the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Pipeline(modeling_steps)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll fit it to the data we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model1` now acts like a `KNeighborsClassifier()` **plus** it does the transformation required to predict and score the results. For example, if we wanted to see the score on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we pass in **new** data, it will still do the requisite transformations, provided that it sees a column named `'petal_length'` in the incoming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can get predictions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.predict(X_test)[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks for Understanding\n",
    "\n",
    "For the `Pipeline` Checks for Understanding, we will be using a dataset from the University of California at Irvine's Machine Learning Repository on predicting the age of [Abalones](https://en.wikipedia.org/wiki/Abalone), which are a type of marine snail. Manually confirming the age of the Abalone is a very manual and difficult process involving cutting the shell open and using a microscope to count rings. Automating this process saved researchers quite a bit of time!\n",
    "\n",
    "![](./images/abalone.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset can be accessed directly via the [Abalone Dataset](http://archive.ics.uci.edu/ml/datasets/Abalone) page and contains the following features:\n",
    "\n",
    "|Name|Data Type|Measurement|Description|\n",
    "|:---|:---|:---|:---|\n",
    "|Sex|nominal||M, F, and I (infant) |\n",
    "|Length\t|continuous|mm|Longest shell measurement |\n",
    "|Diameter|continuous|mm|perpendicular to length |\n",
    "|Height|continuous|mm|with meat in shell |\n",
    "|Whole weight|continuous|grams\t|whole abalone |\n",
    "|Shucked weight\t|continuous|grams\t|weight of meat |\n",
    "|Viscera weight\t|continuous|grams\t|gut weight (after bleeding) |\n",
    "|Shell weight|continuous|grams|after being dried |\n",
    "|Rings\t\t|integer||+1.5 gives the age in years |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_columns = ['sex', 'length', 'diameter', 'height',\n",
    "                  'whole_weight', 'shucked_weight', 'viscera_weight',\n",
    "                  'shell_weight', 'rings']\n",
    "\n",
    "abalone = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data',\n",
    "                     names=abalone_columns)\n",
    "\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could approach predicting the number of rings in a couple of ways:\n",
    "\n",
    "1. We could treat it like a continuous variable and use a regression technique\n",
    "2. We could attempt to predict each number of rings as if it were its own class\n",
    "3. We could engineer a new feature to predict using `rings` as the starting point. \n",
    "\n",
    "For the ease of learning, we're going to pick the third option and predict whether or not the abalone in question is above the average age in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = abalone[[col for col in abalone.columns if col != 'rings']].copy()\n",
    "rings = abalone['rings'].copy()\n",
    "y = rings.apply(lambda x: 1 if x > rings.mean() else 0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.describe(), '\\n\\n', rings.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding 1 \n",
    "\n",
    "Our target column is stored in `y` and our features are now stored in `X`. Using the `abalone` dataset, please do the following individually:\n",
    "\n",
    "1. Use `train_test_split()` with the following parameters:\n",
    "    - `test_size`: `0.25`\n",
    "    - `random_state`: `20171107`\n",
    "    - Name your dataframes `X_train`, `X_test`, `y_train`, `y_test`\n",
    "2. Using `FeatureExtractor`, `PolynomialFeatures`, `StandardScaler`, and `LogisticRegression`, create a list of steps that:\n",
    "    1. Extracts the `diameter` column using `FeatureExtractor`\n",
    "    2. Creates a set of $\\text{diameter}$, $\\text{diameter}^2$, and $\\text{diameter}^3$ using `PolynomialFeatures` (don't forget to ignore the bias term!)\n",
    "    3. Standardizes those new features using `StandardScaler`\n",
    "    4. Feeds the resulting 3 columns into `LogisticRegression`\n",
    "> Remember, each step needs to be formatted: `('name of step', callable())`\n",
    "3. Create a `Pipeline` object with your list of steps.\n",
    "4. Fit your `Pipeline` object to your `X_train` and `y_train` dataframes.\n",
    "5. **Without refitting your `Pipeline` object**, use it to score `X_test` and `y_test`\n",
    "6. Create predictions using your `Pipeline` object on your test set. Use `confusion_matrix()` and `classification_report()` investigate the goodness of fit of your model. How well is it fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Steps -- `make_pipeline()`\n",
    "\n",
    "If we don't care about the names of each individual step (and we rarely will), we can use the `make_pipeline()` helper function to move steps forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    FeatureExtractor('Diameter'),\n",
    "    PolynomialFeatures(3, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having to create a `list` of steps separately, `make_pipeline()` takes in a series of arguments and creates the pipeline for you, automatically naming the steps as needed. It can be a huge timesaver!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Diversion: Categorical Variables Encoded as Strings\n",
    "\n",
    "So, there are some steps here to modeling the `abalone` dataset that sklearn does not handle particularly elegantly:\n",
    "\n",
    "1. Extracting features from a DataFrame by column name\n",
    "2. Creating dummy features from categorical variables that are encoded as strings\n",
    "\n",
    "Step 1 we've already taken care of with our custom `FeatureExtractor` class. Step 2, however, is a little inelegant. We'll be creating a second custom class, `CategoryConverter()` and using a preprocessing module known as `OneHotEncoder()`\n",
    "\n",
    "### `OneHotEncoder`\n",
    "\n",
    "OneHotEncoder will take a column of data and convert it into dummy variables (one per unique instance in the feature). \n",
    "\n",
    "Two keyword arguments we will want to set are:\n",
    "- `sparse=False`: `OneHotEncoder` defaults to returning a `sparse` matrix after transformation. We want to see dense numpy arrays instead\n",
    "- `handle_unknown='ignore'`: `OneHotEncoder` defaults to raising an error if it sees a value it does not know when transforming. We'll set this to `ignore` that. If `OneHotEncoder` transforms a column with values it has not seen before, it will set each column to `0`, which is what we want. \n",
    "\n",
    "**However**, `OneHotEncoder` will _only_ transform columns that use integers to represent categories (a more user-friendly version of `OneHotEncoder` named `CategoricalEncoder` that will, among other things, gracefully handle strings, is currently under active development (see PR [here](https://github.com/scikit-learn/scikit-learn/pull/9151)) but until then, we need a class that will do the following:\n",
    "\n",
    "1. Take in a Pandas dataframe\n",
    "2. Extract a single column\n",
    "3. When we fit the column: it should assign a unique integer to each string category.\n",
    "4. When we transform the column:\n",
    "    - It should convert each of the string categories to that integer\n",
    "    - It should return that column as a 2D numpy array.\n",
    "\n",
    "In the interests of time, I'm going to define that class here and move forward, but this is a **great opportunity** to, on your own time, investigate what makes this class work.\n",
    "\n",
    "(Also note: this is not a rigorously tested piece of code (certainly not to the standards that sklearn's code is!) -- you should not expect 100% performance from it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoricalExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        self.values = None\n",
    "        \n",
    "    def _create_values(self, indices):\n",
    "        return {ind: i+1 for i, ind in enumerate(indices)}\n",
    "    \n",
    "    def _apply_values(self, row_val):\n",
    "        return self.values.get(row_val, 0)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.values = self._create_values(X[self.column].value_counts().index)\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        col = X[self.column].apply(self._apply_values)\n",
    "        return col.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CategoricalExtractor` (for cases where we have incoming categories) instead of `FeatureExtractor`. Once we do that, we'll pass it directly into `OneHotEncoder` with the two options we identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "\n",
    "pipe.fit(X_train)\n",
    "print(pipe.transform(X_train)[0:5, :])\n",
    "print(X_train[['sex']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Steps -- `FeatureUnion()`\n",
    "\n",
    "`Pipeline` works great if we have a set of steps that we want to do in a direct series. A great application of this is to create a series of steps to apply to one specific column.\n",
    "\n",
    "**However**, we frequently want to do different things to different columns. `FeatureUnion()` lets us take multiple transformers and join the results back into one array.\n",
    "\n",
    "Let's imagine we have two features we want to predict with in the Iris dataset:\n",
    "\n",
    "- `petal_length` as a dummy feature coded 1 if it is above the mean for that column\n",
    "- `petal_width` as both itself and a $\\text{petal_width}^2$ feature.\n",
    "\n",
    "We can create each pipeline seperately, first for `petal_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "petal_length_pipe = make_pipeline(\n",
    "    FeatureExtractor('petal_length'),\n",
    "    Binarizer(iris['petal_length'].mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a second for the `petal_width` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "petal_width_pipe = make_pipeline(\n",
    "    FeatureExtractor('petal_length'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FeatureUnion()` lets me run these in parallel and join the results. `.fit()` will call the `.fit()` method of each of the constituent transformers, and `.transform()` will transform each and then join the results. \n",
    "\n",
    "Just like `Pipeline()`, we need to create a list of these pipelines as a tuple with a name and the step. Each of the pipelines we've created above (`petal_length_pipe` and `petal_width_pipe`) are considered steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = FeatureUnion([\n",
    "    ('petal_length_transformer', petal_length_pipe),\n",
    "    ('petal_width_transformer', petal_width_pipe)\n",
    "])\n",
    "\n",
    "fu.fit(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calling `.transform()` will run each pipeline in parallel on the *same* set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu.transform(iris)[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are joined in the order of the steps provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `Pipeline`, `FeatureUnion` also has a function that removes some of the boilerplate code (`make_union()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = make_union(\n",
    "    petal_length_pipe,\n",
    "    petal_width_pipe\n",
    ")\n",
    "\n",
    "fu.fit(iris)\n",
    "fu.transform(iris)[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check For Understanding 2 (10 minutes)\n",
    "\n",
    "Using the `abalone` dataset:\n",
    "\n",
    "1. Create three pipeline objects (using `make_pipeline()`, doing the following steps:\n",
    "   1. Extract `length`, use `Binarizer` to cut at the average value for length\n",
    "   2. Extract `diameter`, use `PolynomialFeatures` (`include_bias=False`) to create a $\\text{diameter}$ and $\\text{diameter}^2$ feature\n",
    "   3. Extract `height`, use `Binarizer` to cut it at a value of 0.10\n",
    "2. Before checking your work with Python, how many features would these three pipelines make? \n",
    "3. Feed each of these steps into a feature union, using `make_union()`\n",
    "4. Fit and transform `X_train`. What is the shape of this numpy array? Does it match your expectations from question 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The power of `Pipeline` and `FeatureUnion`\n",
    "\n",
    "The true power of `Pipeline` and `FeatureUnion` is that we can **chain them together**.\n",
    "\n",
    "As long as we can break down the feature engineering and modeling into sets of steps that need to happen in parallel and steps that need to happen in series (and provided that every step is an sklearn transformer that provides a `.fit()` and a `.transform()` method), we can map out the entire modeling process **and fit / run it** with one call. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to create a quick model using the `abalone` data. My model will consist of the following steps:\n",
    "\n",
    "1. Data Transformation (in parallel):\n",
    "    - Extract `sex` and create dummy variables (one pipeline)\n",
    "    - Extract `length` and do nothing else (one transformer)\n",
    "    - Extract `width` and do nothing else (one transformer)\n",
    "    - Extract `diameter` and create polynomial features (one pipeline)\n",
    "2. Scale and fit a `RandomForestClassifier` (one pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll create the two multistep extractors (for `sex` and `diameter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_sex_pipeline = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "extract_diameter_pipeline = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll create the feature union that extracts everything and makes one large array with all of my transformed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_transformers = make_union(\n",
    "    extract_sex_pipeline,\n",
    "    FeatureExtractor('length'),\n",
    "    FeatureExtractor('height'),\n",
    "    extract_diameter_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll set up my main modeling pipeline. This has three steps:\n",
    "\n",
    "1. Do all of the feature transformations\n",
    "2. Standard Scale the entire set of data\n",
    "3. Pass into `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeling_pipe = make_pipeline(\n",
    "    feature_transformers,\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll fit and score our `RandomForestClassifier` on the training data, then the test data, then use predictions to see how well we did via a confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modeling_pipe.predict(X_test)\n",
    "\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions),\n",
    "                  columns=['Predicted 0', 'Predicted 1'],\n",
    "                  index=['Actual 0', 'Actual 1']))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that work for nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding 3 (15 minutes)\n",
    "\n",
    "Your goal for the next 15 minutes is to try and hack our model to make our predictions on the test set more accurate. How you choose to do so is up to you (provided that your changes are added to the pipeline), but you can consider doing any of the following:\n",
    "- Modify the `feature_transformers` portion of our pipeline to add in more features or change how they are transformed.\n",
    "- Change the type of model used in the pipeline.\n",
    "- Change the hyperparameters that are used in the model\n",
    "\n",
    "All of the code from above has been copied below to make editing easier for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sex_pipeline = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "extract_diameter_pipeline = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")\n",
    "\n",
    "feature_transformers = make_union(\n",
    "    extract_sex_pipeline,\n",
    "    FeatureExtractor('length'),\n",
    "    FeatureExtractor('height'),\n",
    "    extract_diameter_pipeline\n",
    ")\n",
    "\n",
    "modeling_pipe = make_pipeline(\n",
    "    feature_transformers,\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "modeling_pipe.fit(X_train, y_train)\n",
    "print('Training Set Score:', modeling_pipe.score(X_train, y_train))\n",
    "print('Test Set Score:', modeling_pipe.score(X_test, y_test))\n",
    "predictions = modeling_pipe.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions),\n",
    "                  columns=['Predicted 0', 'Predicted 1'],\n",
    "                  index=['Actual 0', 'Actual 1']))\n",
    "print('\\nClassification Report')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "`Pipeline` and `FeatureUnion` can create a really robust and powerful way to systematically and reproducibly transform and predict models. In other words, these systems allow us to do the following:\n",
    "\n",
    "1. Be explicit about the transformations that our data undergoes as well as the final model type\n",
    "2. Packages it up into one command, dramatically reducing the chance (compared to doing things manually) that user error affects our results.\n",
    "\n",
    "This sort of structure means that Pipelines are typically best suited to a refactoring step -- once we have a model and associated data transformations that we are happy with, we can refactor our code to apply those transformations reproducibly. \n",
    "\n",
    "A bonus section follows this that briefly introduces the other case where `Pipeline` can be very useful -- in letting us use `GridSearchCV` to iterate over many more parameters than just the hyperparameters of one model. This is fairly advanced material (and dives deep into the guts of sklearn) so we will cover it only briefly, if at all, in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bonus**: Applying `GridSearchCV` to Pipelines\n",
    "\n",
    "You do not need to fully master the following content or be able to apply it on your own (and we may not even get to it in class, depending on time). This is optional material to look at and work through if you're interested in some of the guts of sklearn.\n",
    "\n",
    "One of the more fascinating parts about `Pipelines` is that they can be recognized as estimators just like any of your model objects. One wrinkle is that we will need to change how we write out our dictionary of parameters to grid search over. Let's take the pipeline we were working with the `abalone` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_sex_pipeline = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "extract_diameter_pipeline = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")\n",
    "\n",
    "feature_transformers = make_union(\n",
    "    extract_sex_pipeline,\n",
    "    FeatureExtractor('length'),\n",
    "    FeatureExtractor('height'),\n",
    "    extract_diameter_pipeline\n",
    ")\n",
    "\n",
    "modeling_pipe = make_pipeline(\n",
    "    feature_transformers,\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at `modeling_pipe` we can see the names of each of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're most interested in are the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use these keys in the grid of parameters we pass to `GridSearchCV`. Originally, when we were only fitting an estimator, we would pass them in like this:\n",
    "\n",
    "```python\n",
    "params_grid = {'NAMEOFHYPERPARAMETER': [value of hyper parameters]\n",
    "```\n",
    "\n",
    "Now that we have multiple steps, we need to preface each of the entries in that dictionary with which step has that hyperparameter first:\n",
    "\n",
    "```python\n",
    "params_grid = {'STEPNAME__HYPERPARAMETER': [values]\n",
    "```\n",
    "\n",
    "We'll separate each of the steps with `__` (double underscore).\n",
    "\n",
    "A good way to think about how to craft this is like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.named_steps['randomforestclassifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any of the hyperparameters here, we would want to set up our params grid like this:\n",
    "\n",
    "```python\n",
    "params_grid = {'randomforestclassifier__n_estimators': [10, 100, 1000]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to change the items in the `featureunion` step, it's a little trickier. We can see all of the steps through the following view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipe.named_steps['featureunion'].transformer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to continue diving down named steps until we found the actual step we wanted:\n",
    "\n",
    "For example, if we wanted to grid search and see if it would be better to have 1, 2, or 3 polynomial features for `diameter`, we would need to traverse down:\n",
    "\n",
    "(notice that we're looking at the **first** entry in each of these tuples!)\n",
    "\n",
    "1. The named step in the final pipeline: `featureunion`\n",
    "2. The next step that contains the transformer: `pipeline-2`\n",
    "3. Inside of `pipeline-2` is the transformer that contains the thing we want to tweak (`PolynomialFeatures`): `polynomialfeatures`\n",
    "\n",
    "That means our entry in the paramter_grid would be:\n",
    "\n",
    "```python\n",
    "params_grid = {'featureunion__pipeline-2__polynomialfeatures__degree': [1, 2, 3]}\n",
    "```\n",
    "\n",
    "where each named step is printed, in order, and then separated by double underscores `__` until the actual hyperparameter.\n",
    "\n",
    "It's a little messy, but this lets `GridSearchCV` optimize a number of things at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one final example. Again, this is _fairly_ advanced and included more as a sneak peek for what sklearn is capable of (not for you to be able to replicate on your own immediately). \n",
    "\n",
    "Here, we're going to let `GridSearchCV` optimize the following choices for us:\n",
    "\n",
    "1. How many numeric features to keep, using `SelectKBest`\n",
    "2. Whether or not to use StandardScaler\n",
    "3. Whether to use one of the three following models:\n",
    "    - `RandomForestClassifier` (at `n_estimators: 1000`)\n",
    "    - `KNeighborsClassifier` (at `n_neighbors: 5`)\n",
    "    - `LogisticRegression` (at default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sex_pipeline = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "feature_transformer = make_union(\n",
    "    extract_sex_pipeline,\n",
    "    FeatureExtractor('length'),\n",
    "    FeatureExtractor('diameter'),\n",
    "    FeatureExtractor('height'),\n",
    "    FeatureExtractor('whole_weight'),\n",
    "    FeatureExtractor('shucked_weight'),\n",
    "    FeatureExtractor('viscera_weight'),\n",
    "    FeatureExtractor('shell_weight')\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('feature', feature_transformer),\n",
    "    ('selectkbest', SelectKBest(score_func=f_classif)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'selectkbest__k': range(2, 10, 2), # tries 2-10 selected columns \n",
    "    'scaler': [None, StandardScaler()], # tries StandardScaler or skips step\n",
    "    'clf': [RandomForestClassifier(n_estimators=1000),\n",
    "           KNeighborsClassifier(n_neighbors=5),\n",
    "           LogisticRegression()],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(model, grid, verbose=1, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently a `RandomForestClassifier` with no scaling and 8 features selected by `SelectKBest` scored the best on our training set. How does this look against our test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Because we passed in a `Pipeline` object into `GridSearchCV`, the `.best_estimator_` in the grid search is the entire pipeline -- making it really easy to check our results against a holdout set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions),\n",
    "                  columns=['Predicted 0', 'Predicted 1'],\n",
    "                  index=['Actual 0', 'Actual 1']))\n",
    "print('\\nClassification Report')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to swap between different modeling techniques once we have a good idea what a few of them work best as and need to make a final determination across them!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
